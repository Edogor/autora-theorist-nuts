{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmZ1RNydtCA1"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this benchmarking challenge, you will be tasked to create an [AutoRA theorist](https://autoresearch.github.io/autora/theorist/) to discover equations that best describe a given data set.\n",
        "\n",
        "You can use this notebook to benchmark your theorist based on 3 different datasets from 3 different cogntive models, respectively.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cvka_h_k3KhR"
      },
      "source": [
        "## Benchmarking Challenge Rules\n",
        "\n",
        "- All contributing teams must publish a pip package of their theorist. We recommend using a pre-release.\n",
        "- The package name must be ``autora-theorist-yourtheorist``\n",
        "- The **theorist must have a fit and predict function**, and it must be **compatible with the synthetic models** in this notebook. We will call the following code to evaluate the theorists, except that we won't be using the Weber-Fechner-Law:\n",
        "\n",
        "```python\n",
        "pip install --pre <autora-theorist-yourtheorist>\n",
        "\n",
        "from autora.theorist.yourtheorist import yourtheorist\n",
        "from autora.experiment_runner.synthetic.psychophysics.weber_fechner_law import weber_fechner_law # this will be a secret ground truth model\n",
        "\n",
        "experiment_runner = weber_fechner_law()\n",
        "theorist = yourtheorist()\n",
        "\n",
        "# generate all conditions\n",
        "conditions = experiment_runner.domain()\n",
        "\n",
        "# generate all corresponding observations\n",
        "experiment_data = experiment_runner.run(conditions, added_noise=0.01)\n",
        "\n",
        "# get the name of the independent and independent variables\n",
        "ivs = [iv.name for iv in experiment_runner.variables.independent_variables]\n",
        "dvs = [dv.name for dv in experiment_runner.variables.dependent_variables]\n",
        "\n",
        "# extract the dependent variable (observations) from experiment data\n",
        "conditions = experiment_data[ivs]\n",
        "observations = experiment_data[dvs]\n",
        "\n",
        "# split into train and test datasets\n",
        "conditions_train, conditions_test, observations_train, observations_test = train_test_split(conditions, observations)\n",
        "\n",
        "# fit theorist\n",
        "theorist.fit(conditions_train, observations_train)\n",
        "\n",
        "# compute prediction for validation set\n",
        "predictions = theorist.predict(conditions_test)\n",
        "\n",
        "# evaluate theorist performance\n",
        "error = (predictions - observations_test).pow(2)\n",
        "error = error.mean()\n",
        "\n",
        "theorist.print_eqn()\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "- Theorist also **must have a function ``print_eqn`` returning the discovered equation as a string.**\n",
        "- The discovered equation **may not include more than 40 symbols/function elements**.\n",
        "- Teams will be ranked based on the fit to two ground-truth models. Observations will be sampled across the entire domain of the ground-truth model. The rank will be accumulated across all benchmarking challenges. The team with the best accumulated rank will win a prize at the end of the workshop.\n",
        "- The teams must provide a **brief presentation** (less than 5 minutes) of their theorist just before the final evaluation. You may use [this template for your slides](https://docs.google.com/presentation/d/1esmYO09ehiGOkzBsBAEf_wCfx2JrlmWwP7dyfpu0dEM/edit?usp=sharing).\n",
        "\n",
        "*Hint: You may not use a search space including more than the following functions and operators.*\n",
        "  - +\n",
        "  - -\n",
        "  - *\n",
        "  - /\n",
        "  - e^x\n",
        "  - ln x\n",
        "  - x^c\n",
        "  - c (constants may appear)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h--sPj7T3Nqc"
      },
      "source": [
        "## Grading\n",
        "\n",
        "- Due date: **August 30**\n",
        "- Submission: Through ``Stud.IP -> Tasks -> Equation Discovery Challenge``\n",
        "\n",
        "The grading is independent of the outcome of the benchmarking challenge.\n",
        "\n",
        "The following points will be provided:\n",
        "- A demonstration of the theorist in the ``doc/Basic Usage.ipynb``:\n",
        "  -  2 points: Demonstrating how to use the theorist for fitting\n",
        "  -  2 points: Demonstrating how to use the theorist for predicting\n",
        "  -  2 points: Demonstrating how to obtain the equation fit by the theorist\n",
        "  -  4 points: Demonstrating that the theorist can recover at least two ground truth models. (*Hint: You could use the benchmarking part of the notebook below as a starting point*).\n",
        "- The documentation of the theorist in ``doc/index.md`` must speak to the following information:\n",
        "  - 2 points: Search Algorithm: a description of the search algorithm, and how the goodness of the equation was determined?\n",
        "  - 2 points: Search Space: which search space was used and how was the search otherwise constrained?\n",
        "- 2 points: The code contains at least two useful unit tests for the theorist method (either doc tests or separate tests)\n",
        "- 1 point: the contributors used issues to track bugs and work on features.\n",
        "- 1 point: the contributors used (helpful) code reviews for their PRs.\n",
        "- 1 point: Unit tests are automatically executed when a pull request is created.\n",
        "- 1 point: The documentation is hosted automatically.\n",
        "\n",
        "Finally, teams must outline the contributions of each team member in their submission on ``Stud.IP -> Tasks -> Equation Discovery Challenge``.\n",
        "\n",
        "In total, you can obtain 20 points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4ICn4w03PdN"
      },
      "source": [
        "## Additional Resources\n",
        "\n",
        "- You can learn more about how to write AutoRA theorists in the [Contributor Guide](https://autoresearch.github.io/autora/contribute/modules/theorist/).\n",
        "\n",
        "- You can learn more about how to use AutoRA in the [User Tutorial](https://autoresearch.github.io/autora/tutorials/).\n",
        "\n",
        "- If you want to learn more about AutoRA states, you can check out [this tutorial on using AutoRA states](https://colab.research.google.com/drive/1yK1OBRpPZM1NgTMV9arHlE5j8xXjsGqq?usp=sharing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zowXxLUR3s8I"
      },
      "source": [
        "# Code for Benchmarking your Theorist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an-QPQhcljHa"
      },
      "source": [
        "## Installation\n",
        "\n",
        "*Hint: You may want to add ``autora`` as a development dependency in your ``pyproject.tml``.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8nzJmQwQlkh_"
      },
      "outputs": [],
      "source": [
        "#%%capture\n",
        "#pip install autora\n",
        "#!pip install autora[all-theorists]\n",
        "#HEY guys this is my test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ty6AuO0iloHs"
      },
      "outputs": [],
      "source": [
        "# autora state\n",
        "from autora.state import StandardState, on_state, Delta\n",
        "\n",
        "# experiment_runner\n",
        "from autora.experiment_runner.synthetic.psychophysics.weber_fechner_law import weber_fechner_law\n",
        "from autora.experiment_runner.synthetic.psychophysics.stevens_power_law import stevens_power_law\n",
        "from autora.experiment_runner.synthetic.economics.expected_value_theory import expected_value_theory\n",
        "\n",
        "# experimentalist\n",
        "from autora.experimentalist.grid import grid_pool\n",
        "from autora.experimentalist.random import random_pool, random_sample\n",
        "\n",
        "# data handling\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwlTublQiWMp"
      },
      "source": [
        "## Benchmarking Function\n",
        "\n",
        "We will use the following function for benchmarking our model for any given experiment runner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "N-EDlyo-iVDf"
      },
      "outputs": [],
      "source": [
        "def benchmark(experiment_runner, theorist):\n",
        "\n",
        "  # generate all conditions\n",
        "  conditions = experiment_runner.domain()\n",
        "\n",
        "  # generate all corresponding observations\n",
        "  experiment_data = experiment_runner.run(conditions, added_noise=0.01)\n",
        "\n",
        "  # get the name of the independent and independent variables\n",
        "  ivs = [iv.name for iv in experiment_runner.variables.independent_variables]\n",
        "  dvs = [dv.name for dv in experiment_runner.variables.dependent_variables]\n",
        "\n",
        "  # extract the dependent variable (observations) from experiment data\n",
        "  conditions = experiment_data[ivs]\n",
        "  observations = experiment_data[dvs]\n",
        "\n",
        "  # split into train and test datasets\n",
        "  conditions_train, conditions_test, observations_train, observations_test = train_test_split(conditions, observations)\n",
        "\n",
        "  print(\"#### EXPERIMENT CONDITIONS (X):\")\n",
        "  print(type(conditions))\n",
        "  print(conditions)\n",
        "  print(\"#### EXPERIMENT OBSERVATIONS (Y):\")\n",
        "  print(observations)\n",
        "\n",
        "  # fit theorist\n",
        "  theorist.fit(conditions_train, observations_train)\n",
        "\n",
        "  # compute prediction for validation set\n",
        "  predictions = theorist.predict(conditions_test)\n",
        "\n",
        "  # evaluate theorist performance\n",
        "  error = (predictions - observations_test).pow(2)\n",
        "  error = error.mean()\n",
        "\n",
        "  print(\"#### IDENTIFIED EQUATION:\")\n",
        "  print(theorist.print_eqn())\n",
        "\n",
        "  print(\"#### VALIDATION SET MSE:\")\n",
        "  print(error)\n",
        "\n",
        "  experiment_runner.plotter(model=theorist)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'NutsTheorist' from 'autora.theorist.nuts' (/workspaces/autora-theorist-nuts/src/autora/theorist/nuts/__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mautora\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtheorist\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnuts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NutsTheorist, NutsConfig\n\u001b[32m      2\u001b[39m theorist = NutsTheorist(NutsConfig(csv_path=\u001b[33m\"\u001b[39m\u001b[33mruns.csv\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m      3\u001b[39m theorist.fit(X, y, var_names=[\u001b[33m\"\u001b[39m\u001b[33mV_A\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mP_A\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mV_B\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mP_B\u001b[39m\u001b[33m\"\u001b[39m])\n",
            "\u001b[31mImportError\u001b[39m: cannot import name 'NutsTheorist' from 'autora.theorist.nuts' (/workspaces/autora-theorist-nuts/src/autora/theorist/nuts/__init__.py)"
          ]
        }
      ],
      "source": [
        "from autora.theorist.nuts import NutsTheorist, NutsConfig\n",
        "theorist = NutsTheorist(NutsConfig(csv_path=\"runs.csv\"))\n",
        "theorist.fit(X, y, var_names=[\"V_A\",\"P_A\",\"V_B\",\"P_B\"])\n",
        "print(theorist.print_eqn())\n",
        "yhat = theorist.predict(X)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qP0mP9-fdWQ"
      },
      "source": [
        "## Dummy Theorist\n",
        "\n",
        "For demonstration purposes, we will use this dummy theorist below. However, to effectively benchmark *your* equation discovery method, you may instead import your theorist from ``.src.theorist`` and use it instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCyDW3SYvCNu"
      },
      "source": [
        "**Note**: Once you implemented your theorist in ``src/autora/theorist/autora_theorist_yourtheorist`` then you can add it as a theorist instead:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fQsr51ZvQh5"
      },
      "outputs": [],
      "source": [
        "# from autora.theorist.autora_theorist_polynomial import ExampleRegressor\n",
        "\n",
        "# my_theorist = ExampleRegressor()\n",
        "\n",
        "#my_theorist = dummy_theorist # remove this once you implemented your own theorist\n",
        "\n",
        "#from autora.theorist.nuts import SimpleLinearTheorist\n",
        "#my_theorist = SimpleLinearTheorist()\n",
        "\n",
        "from autora.theorist.nuts import NutsTheorists\n",
        "my_theorist = model = NutsTheorists(\n",
        "    population_size=300,\n",
        "    n_generation=4,\n",
        "    constant_search=\"random\",   # wichtig für Speed\n",
        "    constant_samples=400,       # 200–500 guter Start\n",
        "    verbose=True,\n",
        "    \n",
        ")\n",
        "my_theorist.set_time_limit(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euv_8ldEspqC"
      },
      "source": [
        "## Ground-Truth Models for Benchmarking\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qp9BfCdfaFb9"
      },
      "source": [
        "### Steven's Power Law\n",
        "\n",
        "Steven's power law describes the relationship between a stimulus's intensity $S$ ($range: [0.01, 5.00]$) and its perceived magnitude $y$. According to this law, humans are less sensitive to changes in high-intensity stimuli compared to low-intensity ones, leading to a power-law relationship between stimulus intensity and perceived magnitude:\n",
        "\n",
        "\n",
        "$\\text{perceived intensity} = {S}^\\alpha$\n",
        "\n",
        "where $\\alpha = 0.80$, resulting in diminishing effects of increases in stimulus intensity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Is5xCDXDdcN5",
        "outputId": "cfdd1bd9-17fe-4739-a638-d1c95755fbaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### EXPERIMENT CONDITIONS (X):\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "           S\n",
            "0   0.010000\n",
            "1   0.060404\n",
            "2   0.110808\n",
            "3   0.161212\n",
            "4   0.211616\n",
            "..       ...\n",
            "95  4.798384\n",
            "96  4.848788\n",
            "97  4.899192\n",
            "98  4.949596\n",
            "99  5.000000\n",
            "\n",
            "[100 rows x 1 columns]\n",
            "#### EXPERIMENT OBSERVATIONS (Y):\n",
            "    perceived_intensity\n",
            "0              0.019206\n",
            "1              0.105223\n",
            "2              0.169384\n",
            "3              0.227655\n",
            "4              0.282656\n",
            "..                  ...\n",
            "95             3.519961\n",
            "96             3.520833\n",
            "97             3.578098\n",
            "98             3.595828\n",
            "99             3.608469\n",
            "\n",
            "[100 rows x 1 columns]\n",
            "\n",
            "=== Generation 1/4 ===\n",
            "Evaluated: exp(S), MSE: 2222.27692, constants: {}, complexity: 2\n",
            "Evaluated: S, MSE: 0.45550, constants: {}, complexity: 1\n",
            "Evaluated: c1, MSE: 1.06024, constants: {'c1': 1.996788586159722}, complexity: 1\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# run benchmark\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mbenchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_runner\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mstevens_power_law\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheorist\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmy_theorist\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mbenchmark\u001b[39m\u001b[34m(experiment_runner, theorist)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(observations)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# fit theorist\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mtheorist\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconditions_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# compute prediction for validation set\u001b[39;00m\n\u001b[32m     30\u001b[39m predictions = theorist.predict(conditions_test)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/autora-theorist-nuts/src/autora/theorist/nuts/__init__.py:659\u001b[39m, in \u001b[36mNutsTheorists.fit\u001b[39m\u001b[34m(self, conditions, observations, time_limit_min)\u001b[39m\n\u001b[32m    657\u001b[39m means = []\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m b_i, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(biomes):\n\u001b[32m--> \u001b[39m\u001b[32m659\u001b[39m     m = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evolve_biome_one_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    660\u001b[39m     means.append(m)\n\u001b[32m    661\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/autora-theorist-nuts/src/autora/theorist/nuts/__init__.py:545\u001b[39m, in \u001b[36mNutsTheorists._evolve_biome_one_gen\u001b[39m\u001b[34m(self, biome, X, y)\u001b[39m\n\u001b[32m    543\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_time() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stop_requested:\n\u001b[32m    544\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m545\u001b[39m     pop_scores.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate_tree_mse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pop_scores:\n\u001b[32m    548\u001b[39m     \u001b[38;5;66;03m# Nichts bewertet -> sauber zurück\u001b[39;00m\n\u001b[32m    549\u001b[39m     \u001b[38;5;28mself\u001b[39m.mutation_rate, \u001b[38;5;28mself\u001b[39m.complexity_penalty, \u001b[38;5;28mself\u001b[39m.tournament_size = saved\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/autora-theorist-nuts/src/autora/theorist/nuts/__init__.py:277\u001b[39m, in \u001b[36mNutsTheorists._evaluate_tree_mse\u001b[39m\u001b[34m(self, tree, conditions, observations)\u001b[39m\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tree, \u001b[38;5;28mfloat\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33minf\u001b[39m\u001b[33m\"\u001b[39m), {k: \u001b[32m1.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.constant_names}\n\u001b[32m    276\u001b[39m \u001b[38;5;66;03m# Ausdruck -> Template -> compile (cached)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m eq_str = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tree_translate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree\u001b[49m\u001b[43m)\u001b[49m                   \u001b[38;5;66;03m# z. B. \"(S ^ c1) + c2\"\u001b[39;00m\n\u001b[32m    278\u001b[39m eq_template = \u001b[38;5;28mself\u001b[39m._prepare_equation_template(eq_str) \u001b[38;5;66;03m# z. B. \"(S ** c1) + c2\" mit np.*\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/autora-theorist-nuts/src/autora/theorist/nuts/__init__.py:203\u001b[39m, in \u001b[36mNutsTheorists._tree_translate\u001b[39m\u001b[34m(self, tree)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m op == \u001b[33m'\u001b[39m\u001b[33mnp.power\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtranslated[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ^ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtranslated[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtranslated[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtranslated\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[31mIndexError\u001b[39m: list index out of range"
          ]
        }
      ],
      "source": [
        "# run benchmark\n",
        "benchmark(experiment_runner = stevens_power_law(), theorist = my_theorist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI7aHUNOZpbL"
      },
      "source": [
        "### Weber-Fechner-Law\n",
        "\n",
        "The Weber-Fechner law quantifies the minimum change in a stimulus required to be noticeable. Similar to Steven's power law, the greater the intensity of a stimulus, the larger the change needed to be perceivable. This relationship is hypothesized to be proportional to the logarithm of the ratio between the two stimuli:\n",
        "\n",
        "$\\text{perceived intensity} = \\log\\left(\\dfrac{S_1}{S_2}\\right)$\n",
        "\n",
        "\n",
        "where $S_1$ ($range: [0.01, 5.00]$) is the intensity of a physical stimulus (e.g., the luminosity of a lamp), $S_2$ ($range: [0.01, 5.00]$ ) is a reference stimulus (e.g., the luminosity of a background light), and $y$ is the perceived stimulus intensity (e.g. the perception of the lamp's luminosity)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S_oOZXa2eTkz",
        "outputId": "bda58640-d011-4dd0-cc57-06c84248de3a"
      },
      "outputs": [],
      "source": [
        "# run benchmark\n",
        "my_theorist = model = NutsTheorists(\n",
        "    population_size=300,\n",
        "    n_generation=20,\n",
        "    constant_search=\"random\",   # wichtig für Speed\n",
        "    constant_samples=400,       # 200–500 guter Start\n",
        "    verbose=True\n",
        ")\n",
        "my_theorist.set_time_limit(1)\n",
        "benchmark(experiment_runner = weber_fechner_law(), theorist = my_theorist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZGn2XzoaUF0"
      },
      "source": [
        "### Expected Utility Model with Two Choice Options\n",
        "\n",
        "The expected utility model evaluates decision-making under uncertainty, quantifying the expected value of different choices based on their potential outcomes and associated probabilities. The model assumes that individuals aim to maximize their expected utility when faced with two options. Each option has a specific value and probability, influenced by a certain level of noise.\n",
        "\n",
        "For two choice options, the expected value of each option is calculated as follows:\n",
        "\n",
        "$$\n",
        "E_A = V_A \\times P_A\n",
        "$$\n",
        "\n",
        "$$\n",
        "E_B = V_B \\times P_B\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $V_A$ and $V_B$ represent the values of options A and B respectively.\n",
        "- $P_A$ and $P_B$ represent the probabilities associated with these options.\n",
        "\n",
        "The probability of choosing option A $P_{\\text{choose}_A}$ is then determined using the softmax function, which considers the expected values of both options and a choice temperature parameter that influences the sensitivity to differences in expected values:\n",
        "\n",
        "$$\n",
        "P_{\\text{choose}_A} = \\frac{\\exp(E_A / \\beta)}{\\exp(E_A / \\beta) + \\exp(E_B / \\beta)}\n",
        "$$\n",
        "\n",
        "In this model:\n",
        "- $\\beta$ controls the degree of randomness in the choice, with higher values leading to more exploration and lower values leading to more deterministic choices based on the expected values.\n",
        "- The softmax function ensures that the probabilities sum to 1, providing a normalized measure of the likelihood of choosing each option.\n",
        "\n",
        "This model captures the influence of value, probability, and noise on decision-making, reflecting the complexity and variability of human choices under uncertainty.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "i0n8QAWVs19V",
        "outputId": "961cf3bd-174d-42e8-d08f-4d188cb2b4da"
      },
      "outputs": [],
      "source": [
        "# run benchmark\n",
        "my_theorist = model = NutsTheorists(\n",
        "    population_size=300,\n",
        "    n_generation=15,\n",
        "    constant_search=\"random\",   # wichtig für Speed\n",
        "    constant_samples=200,       # 200–500 guter Start\n",
        "    complexity_penalty=0.2,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "benchmark(experiment_runner = expected_value_theory(), theorist = my_theorist)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
